{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import recall_score, make_scorer\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code found from http://www.davidsbatista.net/blog/2018/02/23/model_optimization/\n",
    "class EstimatorSelectionHelper:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv=3, n_jobs=-1, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring, refit=refit,\n",
    "                              return_train_score=True)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': np.mean(scores),\n",
    "                 'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params,**d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            print(k)\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]        \n",
    "                scores.append(r.reshape(len(params),1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params,all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model_params = {\n",
    "    'LogisticRegression' : {\n",
    "        'penalty' : ['l1', 'l2'],\n",
    "        'C' : np.arange(.05, 1, .05) },\n",
    "    'VanillaLogRegression' : {\n",
    "    },\n",
    "    'KNN' : {\n",
    "        'n_neighbors' : np.arange(3, 22, 2) },\n",
    "    'NaiveBayes' : {\n",
    "        'alpha' : np.arange(.05, 2, .05)},\n",
    "    'DecisionTree': {\n",
    "        'max_depth' : [None, 6, 10, 14], \n",
    "        'min_samples_leaf' : [1, 2],\n",
    "        'min_samples_split': [2, 3] },\n",
    "    'BaggedDecisionTree' : {\n",
    "        'n_estimators' : [20, 60, 100] },\n",
    "    'RandomForest' : {\n",
    "        'n_estimators' : [20, 60, 100],\n",
    "        'max_depth' : [None, 2, 6, 10],\n",
    "        'min_samples_split' : [2, 3, 4] },\n",
    "    'ExtraTrees' : {\n",
    "        'n_estimators' : [20, 60, 100],\n",
    "        'max_depth' : [None, 6, 10, 14],\n",
    "        'min_samples_leaf' : [1, 2], \n",
    "        'min_samples_split' : [2, 3], },\n",
    "    'AdaBoost' : {\n",
    "        'n_estimators' : np.arange(100, 151, 25),\n",
    "        'learning_rate' : np.linspace(0.05, 1, 10) },\n",
    "    'GradientBoosting' : {\n",
    "        'n_estimators' : np.arange(5, 150, 15),\n",
    "        'learning_rate' : np.linspace(0.05, 1, 10),\n",
    "        'max_depth' : [1, 2, 3] },\n",
    "    'SVM' : {\n",
    "        'C' : np.arange(0.05, 1, .05),\n",
    "        'kernel' : ['rbf', 'linear'] },\n",
    "     'XGBoost' : {\n",
    "        'n_estimators'  : np.arange(100, 151, 25), \n",
    "        'learning_rate' : np.arange(0.1, 1, .3),\n",
    "        'max_depth' : [3],\n",
    "        'alpha' : np.arange(0, 1, .3),\n",
    "        'lambda' : np.arange(0, 1, .3),\n",
    "        'gamma' : np.arange(0, 1, .3),\n",
    "        'subsample' : [.5],\n",
    "        'n_jobs' : [4],}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chang\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\chang\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\chang\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "sm = SMOTE(sampling_strategy=1,random_state=666)\n",
    "\n",
    "train = pd.read_csv('./data/train_weather.csv')\n",
    "train_dummies = pd.get_dummies(train,drop_first=True,columns=['Species','Street'])\n",
    "y = train_dummies['WnvPresent']\n",
    "X = train_dummies[[col for col in train_dummies.columns if col != 'WnvPresent']]\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X,y,test_size = 0.3, random_state = 666,stratify=y)\n",
    "train_x=scaler.fit_transform(train_x)\n",
    "test_x=scaler.transform(test_x)\n",
    "sampledX,sampledy = sm.fit_sample(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_models = {\n",
    "    'LogisticRegression' : LogisticRegression(random_state = 42),\n",
    "    'VanillaLogRegression' : LogisticRegression(random_state = 42),\n",
    "    'KNN': KNeighborsClassifier(), \n",
    "#     'NaiveBayes' : MultinomialNB(), #does not work with negative vals\n",
    "#     'DecisionTree' : DecisionTreeClassifier(random_state = 42), \n",
    "#     'BaggedDecisionTree' : BaggingClassifier(random_state = 42),\n",
    "#     'RandomForest' : RandomForestClassifier(random_state = 42), \n",
    "#     'ExtraTrees' : ExtraTreesClassifier(random_state = 42), \n",
    "#     'AdaBoost' : AdaBoostClassifier(random_state=42), \n",
    "#     'GradientBoosting' : GradientBoostingClassifier(random_state = 42),\n",
    "#     'SVM' : SVC(random_state=42),\n",
    "#     'XGBoost' : XGBClassifier(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for LogisticRegression.\n",
      "Fitting 3 folds for each of 38 candidates, totalling 114 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   57.9s\n",
      "[Parallel(n_jobs=3)]: Done 114 out of 114 | elapsed:  2.6min finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for VanillaLogRegression.\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for KNN.\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  30 out of  30 | elapsed:  5.7min finished\n"
     ]
    }
   ],
   "source": [
    "rec=make_scorer(recall_score,average='binary',pos_label=1)\n",
    "search = EstimatorSelectionHelper(classifier_models, classifier_model_params)\n",
    "search.fit(sampledX,sampledy, scoring=rec, n_jobs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then score our different models and output our results to a csv for archival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "VanillaLogRegression\n",
      "KNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chang\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:49: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score1=search.score_summary(sort_by='mean_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "score1=pd.read_csv(r'.\\data\\score3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chang\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "score1=pd.concat([score1,score2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "score1.to_csv(r'.\\data\\score3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator</th>\n",
       "      <th>min_score</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>max_score</th>\n",
       "      <th>std_score</th>\n",
       "      <th>C</th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>penalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.939303</td>\n",
       "      <td>0.951636</td>\n",
       "      <td>0.965992</td>\n",
       "      <td>0.0109895</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.939303</td>\n",
       "      <td>0.950632</td>\n",
       "      <td>0.956952</td>\n",
       "      <td>0.00802905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.935428</td>\n",
       "      <td>0.948335</td>\n",
       "      <td>0.959966</td>\n",
       "      <td>0.0100579</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.93672</td>\n",
       "      <td>0.946901</td>\n",
       "      <td>0.952627</td>\n",
       "      <td>0.00721786</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.928971</td>\n",
       "      <td>0.941591</td>\n",
       "      <td>0.950065</td>\n",
       "      <td>0.00909664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.929832</td>\n",
       "      <td>0.940873</td>\n",
       "      <td>0.950495</td>\n",
       "      <td>0.00849502</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.92768</td>\n",
       "      <td>0.936998</td>\n",
       "      <td>0.944899</td>\n",
       "      <td>0.00710075</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.917348</td>\n",
       "      <td>0.934415</td>\n",
       "      <td>0.944468</td>\n",
       "      <td>0.0121319</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.911322</td>\n",
       "      <td>0.93341</td>\n",
       "      <td>0.952217</td>\n",
       "      <td>0.016856</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.904864</td>\n",
       "      <td>0.932981</td>\n",
       "      <td>0.950926</td>\n",
       "      <td>0.020133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.875592</td>\n",
       "      <td>0.890069</td>\n",
       "      <td>0.901851</td>\n",
       "      <td>0.0108884</td>\n",
       "      <td>0.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.873009</td>\n",
       "      <td>0.887916</td>\n",
       "      <td>0.901421</td>\n",
       "      <td>0.0116413</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.871287</td>\n",
       "      <td>0.886481</td>\n",
       "      <td>0.901421</td>\n",
       "      <td>0.0123032</td>\n",
       "      <td>0.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.869996</td>\n",
       "      <td>0.88562</td>\n",
       "      <td>0.900129</td>\n",
       "      <td>0.0123272</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.870426</td>\n",
       "      <td>0.884903</td>\n",
       "      <td>0.898407</td>\n",
       "      <td>0.0114439</td>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.869996</td>\n",
       "      <td>0.884185</td>\n",
       "      <td>0.898407</td>\n",
       "      <td>0.011599</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.883754</td>\n",
       "      <td>0.899268</td>\n",
       "      <td>0.0121623</td>\n",
       "      <td>0.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.869996</td>\n",
       "      <td>0.883324</td>\n",
       "      <td>0.897546</td>\n",
       "      <td>0.0112653</td>\n",
       "      <td>0.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.869996</td>\n",
       "      <td>0.883324</td>\n",
       "      <td>0.898407</td>\n",
       "      <td>0.0116652</td>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.8743</td>\n",
       "      <td>0.883323</td>\n",
       "      <td>0.896685</td>\n",
       "      <td>0.00963998</td>\n",
       "      <td>0.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.882893</td>\n",
       "      <td>0.898407</td>\n",
       "      <td>0.0118758</td>\n",
       "      <td>0.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.869135</td>\n",
       "      <td>0.882749</td>\n",
       "      <td>0.898407</td>\n",
       "      <td>0.0120375</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.869135</td>\n",
       "      <td>0.882462</td>\n",
       "      <td>0.897116</td>\n",
       "      <td>0.0114616</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.882462</td>\n",
       "      <td>0.897116</td>\n",
       "      <td>0.0113158</td>\n",
       "      <td>0.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>VanillaLogRegression</td>\n",
       "      <td>0.868704</td>\n",
       "      <td>0.882319</td>\n",
       "      <td>0.897116</td>\n",
       "      <td>0.011629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.882319</td>\n",
       "      <td>0.897116</td>\n",
       "      <td>0.0113399</td>\n",
       "      <td>0.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.868704</td>\n",
       "      <td>0.882175</td>\n",
       "      <td>0.897546</td>\n",
       "      <td>0.0118511</td>\n",
       "      <td>0.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.869135</td>\n",
       "      <td>0.882175</td>\n",
       "      <td>0.897116</td>\n",
       "      <td>0.0115019</td>\n",
       "      <td>0.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.869996</td>\n",
       "      <td>0.882175</td>\n",
       "      <td>0.896685</td>\n",
       "      <td>0.0110199</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.869996</td>\n",
       "      <td>0.882032</td>\n",
       "      <td>0.895824</td>\n",
       "      <td>0.0106174</td>\n",
       "      <td>0.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.869135</td>\n",
       "      <td>0.882032</td>\n",
       "      <td>0.897116</td>\n",
       "      <td>0.0115274</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.882032</td>\n",
       "      <td>0.897977</td>\n",
       "      <td>0.0118569</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.870426</td>\n",
       "      <td>0.882032</td>\n",
       "      <td>0.897116</td>\n",
       "      <td>0.0111702</td>\n",
       "      <td>0.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.869135</td>\n",
       "      <td>0.881888</td>\n",
       "      <td>0.895824</td>\n",
       "      <td>0.010928</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.868704</td>\n",
       "      <td>0.881888</td>\n",
       "      <td>0.896685</td>\n",
       "      <td>0.01148</td>\n",
       "      <td>0.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.870426</td>\n",
       "      <td>0.881745</td>\n",
       "      <td>0.894533</td>\n",
       "      <td>0.00989623</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.868704</td>\n",
       "      <td>0.881745</td>\n",
       "      <td>0.897116</td>\n",
       "      <td>0.0117154</td>\n",
       "      <td>0.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.869996</td>\n",
       "      <td>0.881745</td>\n",
       "      <td>0.895824</td>\n",
       "      <td>0.0106725</td>\n",
       "      <td>0.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.868704</td>\n",
       "      <td>0.881601</td>\n",
       "      <td>0.895394</td>\n",
       "      <td>0.0109144</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.881601</td>\n",
       "      <td>0.895394</td>\n",
       "      <td>0.0106174</td>\n",
       "      <td>0.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.881601</td>\n",
       "      <td>0.895394</td>\n",
       "      <td>0.0106174</td>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.868704</td>\n",
       "      <td>0.881458</td>\n",
       "      <td>0.894963</td>\n",
       "      <td>0.0107334</td>\n",
       "      <td>0.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.868704</td>\n",
       "      <td>0.881458</td>\n",
       "      <td>0.894963</td>\n",
       "      <td>0.0107334</td>\n",
       "      <td>0.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.868704</td>\n",
       "      <td>0.881458</td>\n",
       "      <td>0.894963</td>\n",
       "      <td>0.0107334</td>\n",
       "      <td>0.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.868704</td>\n",
       "      <td>0.881458</td>\n",
       "      <td>0.895824</td>\n",
       "      <td>0.0111303</td>\n",
       "      <td>0.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.868704</td>\n",
       "      <td>0.881314</td>\n",
       "      <td>0.895394</td>\n",
       "      <td>0.0109454</td>\n",
       "      <td>0.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.867843</td>\n",
       "      <td>0.881027</td>\n",
       "      <td>0.895394</td>\n",
       "      <td>0.0112785</td>\n",
       "      <td>0.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.872579</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.889798</td>\n",
       "      <td>0.00707923</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.866552</td>\n",
       "      <td>0.870121</td>\n",
       "      <td>0.873009</td>\n",
       "      <td>0.00267967</td>\n",
       "      <td>0.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               estimator min_score mean_score max_score   std_score     C  \\\n",
       "41                   KNN  0.939303   0.951636  0.965992   0.0109895   NaN   \n",
       "39                   KNN  0.939303   0.950632  0.956952  0.00802905   NaN   \n",
       "42                   KNN  0.935428   0.948335  0.959966   0.0100579   NaN   \n",
       "40                   KNN   0.93672   0.946901  0.952627  0.00721786   NaN   \n",
       "43                   KNN  0.928971   0.941591  0.950065  0.00909664   NaN   \n",
       "44                   KNN  0.929832   0.940873  0.950495  0.00849502   NaN   \n",
       "45                   KNN   0.92768   0.936998  0.944899  0.00710075   NaN   \n",
       "46                   KNN  0.917348   0.934415  0.944468   0.0121319   NaN   \n",
       "47                   KNN  0.911322    0.93341  0.952217    0.016856   NaN   \n",
       "48                   KNN  0.904864   0.932981  0.950926    0.020133   NaN   \n",
       "1     LogisticRegression  0.875592   0.890069  0.901851   0.0108884  0.05   \n",
       "3     LogisticRegression  0.873009   0.887916  0.901421   0.0116413   0.1   \n",
       "5     LogisticRegression  0.871287   0.886481  0.901421   0.0123032  0.15   \n",
       "7     LogisticRegression  0.869996    0.88562  0.900129   0.0123272   0.2   \n",
       "9     LogisticRegression  0.870426   0.884903  0.898407   0.0114439  0.25   \n",
       "11    LogisticRegression  0.869996   0.884185  0.898407    0.011599   0.3   \n",
       "13    LogisticRegression  0.869565   0.883754  0.899268   0.0121623  0.35   \n",
       "17    LogisticRegression  0.869996   0.883324  0.897546   0.0112653  0.45   \n",
       "15    LogisticRegression  0.869996   0.883324  0.898407   0.0116652   0.4   \n",
       "4     LogisticRegression    0.8743   0.883323  0.896685  0.00963998  0.15   \n",
       "21    LogisticRegression  0.869565   0.882893  0.898407   0.0118758  0.55   \n",
       "23    LogisticRegression  0.869135   0.882749  0.898407   0.0120375   0.6   \n",
       "19    LogisticRegression  0.869135   0.882462  0.897116   0.0114616   0.5   \n",
       "37    LogisticRegression  0.869565   0.882462  0.897116   0.0113158  0.95   \n",
       "38  VanillaLogRegression  0.868704   0.882319  0.897116    0.011629   NaN   \n",
       "35    LogisticRegression  0.869565   0.882319  0.897116   0.0113399   0.9   \n",
       "25    LogisticRegression  0.868704   0.882175  0.897546   0.0118511  0.65   \n",
       "29    LogisticRegression  0.869135   0.882175  0.897116   0.0115019  0.75   \n",
       "10    LogisticRegression  0.869996   0.882175  0.896685   0.0110199   0.3   \n",
       "20    LogisticRegression  0.869996   0.882032  0.895824   0.0106174  0.55   \n",
       "31    LogisticRegression  0.869135   0.882032  0.897116   0.0115274   0.8   \n",
       "6     LogisticRegression  0.869565   0.882032  0.897977   0.0118569   0.2   \n",
       "8     LogisticRegression  0.870426   0.882032  0.897116   0.0111702  0.25   \n",
       "22    LogisticRegression  0.869135   0.881888  0.895824    0.010928   0.6   \n",
       "27    LogisticRegression  0.868704   0.881888  0.896685     0.01148   0.7   \n",
       "18    LogisticRegression  0.870426   0.881745  0.894533  0.00989623   0.5   \n",
       "33    LogisticRegression  0.868704   0.881745  0.897116   0.0117154  0.85   \n",
       "12    LogisticRegression  0.869996   0.881745  0.895824   0.0106725  0.35   \n",
       "30    LogisticRegression  0.868704   0.881601  0.895394   0.0109144   0.8   \n",
       "16    LogisticRegression  0.869565   0.881601  0.895394   0.0106174  0.45   \n",
       "14    LogisticRegression  0.869565   0.881601  0.895394   0.0106174   0.4   \n",
       "24    LogisticRegression  0.868704   0.881458  0.894963   0.0107334  0.65   \n",
       "28    LogisticRegression  0.868704   0.881458  0.894963   0.0107334  0.75   \n",
       "26    LogisticRegression  0.868704   0.881458  0.894963   0.0107334   0.7   \n",
       "36    LogisticRegression  0.868704   0.881458  0.895824   0.0111303  0.95   \n",
       "32    LogisticRegression  0.868704   0.881314  0.895394   0.0109454  0.85   \n",
       "34    LogisticRegression  0.867843   0.881027  0.895394   0.0112785   0.9   \n",
       "2     LogisticRegression  0.872579   0.880597  0.889798  0.00707923   0.1   \n",
       "0     LogisticRegression  0.866552   0.870121  0.873009  0.00267967  0.05   \n",
       "\n",
       "   n_neighbors penalty  \n",
       "41           7     NaN  \n",
       "39           3     NaN  \n",
       "42           9     NaN  \n",
       "40           5     NaN  \n",
       "43          11     NaN  \n",
       "44          13     NaN  \n",
       "45          15     NaN  \n",
       "46          17     NaN  \n",
       "47          19     NaN  \n",
       "48          21     NaN  \n",
       "1          NaN      l2  \n",
       "3          NaN      l2  \n",
       "5          NaN      l2  \n",
       "7          NaN      l2  \n",
       "9          NaN      l2  \n",
       "11         NaN      l2  \n",
       "13         NaN      l2  \n",
       "17         NaN      l2  \n",
       "15         NaN      l2  \n",
       "4          NaN      l1  \n",
       "21         NaN      l2  \n",
       "23         NaN      l2  \n",
       "19         NaN      l2  \n",
       "37         NaN      l2  \n",
       "38         NaN     NaN  \n",
       "35         NaN      l2  \n",
       "25         NaN      l2  \n",
       "29         NaN      l2  \n",
       "10         NaN      l1  \n",
       "20         NaN      l1  \n",
       "31         NaN      l2  \n",
       "6          NaN      l1  \n",
       "8          NaN      l1  \n",
       "22         NaN      l1  \n",
       "27         NaN      l2  \n",
       "18         NaN      l1  \n",
       "33         NaN      l2  \n",
       "12         NaN      l1  \n",
       "30         NaN      l1  \n",
       "16         NaN      l1  \n",
       "14         NaN      l1  \n",
       "24         NaN      l1  \n",
       "28         NaN      l1  \n",
       "26         NaN      l1  \n",
       "36         NaN      l1  \n",
       "32         NaN      l1  \n",
       "34         NaN      l1  \n",
       "2          NaN      l1  \n",
       "0          NaN      l1  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "score1=score1.sort_values('mean_score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = score1[score1['mean_score'] == score1.groupby('estimator')['mean_score'].transform('max')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>alpha</th>\n",
       "      <th>estimator</th>\n",
       "      <th>gamma</th>\n",
       "      <th>kernel</th>\n",
       "      <th>lambda</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_score</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_score</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>n_jobs</th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>penalty</th>\n",
       "      <th>std_score</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.991387</td>\n",
       "      <td>0.984927</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.972868</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008534</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.986219</td>\n",
       "      <td>0.971433</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.945306</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018528</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.985788</td>\n",
       "      <td>0.959231</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.921189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.027595</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BaggedDecisionTree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.991387</td>\n",
       "      <td>0.957221</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.894057</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.044713</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.951766</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.879845</td>\n",
       "      <td>150.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.050858</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.261111</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.987080</td>\n",
       "      <td>0.949038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.875538</td>\n",
       "      <td>140.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.051983</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KNN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.955642</td>\n",
       "      <td>0.942578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.920758</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015529</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.788889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.964255</td>\n",
       "      <td>0.931668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.867356</td>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.045477</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SVM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.941860</td>\n",
       "      <td>0.923629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.890612</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.023389</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.855154</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.003793</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        C  alpha           estimator  gamma kernel  lambda  learning_rate  \\\n",
       "13    NaN    NaN          ExtraTrees    NaN    NaN     NaN            NaN   \n",
       "25    NaN    NaN        RandomForest    NaN    NaN     NaN            NaN   \n",
       "28    NaN    NaN        DecisionTree    NaN    NaN     NaN            NaN   \n",
       "14    NaN    NaN  BaggedDecisionTree    NaN    NaN     NaN            NaN   \n",
       "470   NaN    0.0             XGBoost    0.3    NaN     0.9       0.400000   \n",
       "824   NaN    NaN    GradientBoosting    NaN    NaN     NaN       0.261111   \n",
       "64    NaN    NaN                 KNN    NaN    NaN     NaN            NaN   \n",
       "153   NaN    NaN            AdaBoost    NaN    NaN     NaN       0.788889   \n",
       "172  0.90    NaN                 SVM    NaN    rbf     NaN            NaN   \n",
       "135  0.15    NaN  LogisticRegression    NaN    NaN     NaN            NaN   \n",
       "\n",
       "     max_depth  max_score  mean_score  min_samples_leaf  min_samples_split  \\\n",
       "13         NaN   0.991387    0.984927               2.0                2.0   \n",
       "25        10.0   0.986219    0.971433               NaN                3.0   \n",
       "28        14.0   0.985788    0.959231               1.0                2.0   \n",
       "14         NaN   0.991387    0.957221               NaN                NaN   \n",
       "470        3.0   0.988372    0.951766               NaN                NaN   \n",
       "824        3.0   0.987080    0.949038               NaN                NaN   \n",
       "64         NaN   0.955642    0.942578               NaN                NaN   \n",
       "153        NaN   0.964255    0.931668               NaN                NaN   \n",
       "172        NaN   0.941860    0.923629               NaN                NaN   \n",
       "135        NaN   0.860465    0.855154               NaN                NaN   \n",
       "\n",
       "     min_score  n_estimators  n_jobs  n_neighbors penalty  std_score  \\\n",
       "13    0.972868          60.0     NaN          NaN     NaN   0.008534   \n",
       "25    0.945306         100.0     NaN          NaN     NaN   0.018528   \n",
       "28    0.921189           NaN     NaN          NaN     NaN   0.027595   \n",
       "14    0.894057         100.0     NaN          NaN     NaN   0.044713   \n",
       "470   0.879845         150.0     4.0          NaN     NaN   0.050858   \n",
       "824   0.875538         140.0     NaN          NaN     NaN   0.051983   \n",
       "64    0.920758           NaN     NaN          3.0     NaN   0.015529   \n",
       "153   0.867356         150.0     NaN          NaN     NaN   0.045477   \n",
       "172   0.890612           NaN     NaN          NaN     NaN   0.023389   \n",
       "135   0.851852           NaN     NaN          NaN      l2   0.003793   \n",
       "\n",
       "     subsample  \n",
       "13         NaN  \n",
       "25         NaN  \n",
       "28         NaN  \n",
       "14         NaN  \n",
       "470        0.5  \n",
       "824        NaN  \n",
       "64         NaN  \n",
       "153        NaN  \n",
       "172        NaN  \n",
       "135        NaN  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.groupby('estimator').head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = {\n",
    "    'XGB' : XGBClassifier(gamma=0.3,learning_rate=0.4,max_depth=3,n_estimators=150,subsample=0.5),\n",
    "    'ADA' : AdaBoostClassifier(learning_rate=0.788889,n_estimators=150),\n",
    "    'GBoost' : GradientBoostingClassifier(learning_rate=0.261111,max_depth=3,n_estimators=140),\n",
    "    'SVC' : SVC(kernel='rbf'),\n",
    "    'DecisionTree' : DecisionTreeClassifier(max_depth=None,min_samples_leaf=1,min_samples_split=2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      2987\n",
      "           1       0.27      0.24      0.25       165\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      3152\n",
      "   macro avg       0.61      0.60      0.61      3152\n",
      "weighted avg       0.92      0.93      0.92      3152\n",
      "\n",
      "ADA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94      2987\n",
      "           1       0.24      0.50      0.32       165\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      3152\n",
      "   macro avg       0.60      0.70      0.63      3152\n",
      "weighted avg       0.93      0.89      0.91      3152\n",
      "\n",
      "GBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      2987\n",
      "           1       0.29      0.32      0.30       165\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      3152\n",
      "   macro avg       0.62      0.64      0.63      3152\n",
      "weighted avg       0.93      0.92      0.92      3152\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chang\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.84      0.90      2987\n",
      "           1       0.16      0.53      0.24       165\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      3152\n",
      "   macro avg       0.56      0.68      0.57      3152\n",
      "weighted avg       0.93      0.83      0.87      3152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e_name,estimator in estimators.items():\n",
    "    estimator.fit(sampledX,sampledy)\n",
    "    pred=estimator.predict(test_x)\n",
    "    print(e_name)\n",
    "    print(classification_report(test_y,pred))\n",
    "    recall_score(test_y,pred,average='binary',pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96      2989\n",
      "           1       0.21      0.24      0.23       163\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      3152\n",
      "   macro avg       0.59      0.60      0.59      3152\n",
      "weighted avg       0.92      0.92      0.92      3152\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2392638036809816"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt=\n",
    "dt.fit(sampledX,sampledy)\n",
    "pred=dt.predict(test_x)\n",
    "print(classification_report(test_y,pred))\n",
    "recall_score(test_y,pred,average='binary',pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93      2989\n",
      "           1       0.17      0.37      0.23       163\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      3152\n",
      "   macro avg       0.56      0.63      0.58      3152\n",
      "weighted avg       0.92      0.87      0.89      3152\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.36809815950920244"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn=KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(sampledX,sampledy)\n",
    "pred=knn.predict(test_x)\n",
    "print(classification_report(test_y,pred))\n",
    "recall_score(test_y,pred,average='binary',pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96      2989\n",
      "           1       0.34      0.38      0.36       163\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      3152\n",
      "   macro avg       0.65      0.67      0.66      3152\n",
      "weighted avg       0.93      0.93      0.93      3152\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3803680981595092"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et=ExtraTreesClassifier(min_samples_leaf=2,min_samples_split=2,n_estimators=60)\n",
    "et.fit(sampledX,sampledy)\n",
    "pred=et.predict(test_x)\n",
    "print(classification_report(test_y,pred))\n",
    "recall_score(test_y,pred,average='binary',pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94      2989\n",
      "           1       0.24      0.53      0.33       163\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      3152\n",
      "   macro avg       0.60      0.72      0.63      3152\n",
      "weighted avg       0.93      0.89      0.91      3152\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5276073619631901"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf=RandomForestClassifier(max_depth=10,min_samples_split=2,n_estimators=100)\n",
    "rf.fit(sampledX,sampledy)\n",
    "pred=rf.predict(test_x)\n",
    "print(classification_report(test_y,pred))\n",
    "recall_score(test_y,pred,average='binary',pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chang\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.80      0.88      2989\n",
      "           1       0.15      0.67      0.25       163\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      3152\n",
      "   macro avg       0.57      0.74      0.56      3152\n",
      "weighted avg       0.94      0.79      0.85      3152\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6748466257668712"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=LogisticRegression(penalty='l2',C=0.05)\n",
    "lr.fit(sampledX,sampledy)\n",
    "pred=lr.predict(test_x)\n",
    "print(classification_report(test_y,pred))\n",
    "recall_score(test_y,pred,average='binary',pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96      2989\n",
      "           1       0.32      0.26      0.29       163\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      3152\n",
      "   macro avg       0.64      0.62      0.63      3152\n",
      "weighted avg       0.93      0.93      0.93      3152\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.26380368098159507"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd=BaggingClassifier(n_estimators=100)\n",
    "bd.fit(sampledX,sampledy)\n",
    "pred=bd.predict(test_x)\n",
    "print(classification_report(test_y,pred))\n",
    "recall_score(test_y,pred,average='binary',pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
